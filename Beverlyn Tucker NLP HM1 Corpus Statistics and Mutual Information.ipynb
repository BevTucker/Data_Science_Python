{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Mainstream media is a traditional reference for news entities capable of shaping and molding opinions of the general public. The writers capable of reaching out to a different level of audiences. The dissemination of knowledge covers a wide variety of platforms, including television, online, articles, social media, and more.\n",
    "\n",
    "The Limitless capabilities of communication allow news agencies, writers to maximize information delivery. Moreover, benefits potentially include a better-informed and engaged general public. However, the writer can express their feelings or utilizing statistics to avoid biases. The writer can analyze and write an article based on experience, observation, psychology, statistical reports, and feelings, and ideas.\n",
    "\n",
    "This article that is going to instigate whether the writer has change in his writing approached. Choosing two articles written by Russell Carlton for the year 2009 and the year 2018 in the website Baseball Prospectos was he rely heavily on statistics and the psychological side of the report. \n",
    "\n",
    "One of the main themes in NLP is text representation, which is fundamental and indispensable for text-based intelligent information processing. Generally, text representation word frequencies examining the relevance of key-words to documents in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "We will begin loading the nltk and re packages that will allow us to process both texts and do some initial cleaning, such as lowercasing words and stemming.\n",
    "\n",
    "\n",
    "We will first lowercase every word within each corpus. Then, we will remove stopwords from both corpora; however, we will not add any initial stopwords to nltk's stopwords list. If any additional words show up within the corpora that are deemed unnecessary, these words will then be added and will be reprocessed.\n",
    "Finally, we will stem every token within the corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading required packages\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams\n",
    "from nltk.collocations import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "R09 = open(\"Russel1.txt\", \"r\", encoding=\"latin-1\")\n",
    "R18 = open(\"Russel9.txt\", \"r\", encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reading the texts\n",
    "RussY09 = R09.read()\n",
    "RussY18 = R18.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words to check length of corpora\n",
    "RussY09_tokens = nltk.word_tokenize(RussY09)\n",
    "RussY18_tokens = nltk.word_tokenize(RussY18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RussY09: 1369 tokens\n",
      "RussY18: 1650 tokens\n"
     ]
    }
   ],
   "source": [
    "# Print token length\n",
    "print('RussY09: {} tokens'.format(len(RussY09_tokens)))\n",
    "print('RussY18: {} tokens'.format(len(RussY18_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lowercase function that loops through every token in a corpus and returns a list of each lowercased token. \n",
    "def lowercase(text):\n",
    "    token_list = []\n",
    "    for word in text:\n",
    "        word_lower = word.lower()\n",
    "        token_list.append(word_lower)\n",
    "    return(token_list)\n",
    "\n",
    "# Lowercase tokens in each text for easier stop word removal, stemming, and do frequency distribution\n",
    "# for each corpus. \n",
    "td_lower = lowercase(RussY09_tokens)\n",
    "fw_lower = lowercase(RussY18_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "stop_words = stopwords.words('english')\n",
    "td_lower_stop = [word for word in td_lower if word not in stop_words]\n",
    "fw_lower_stop = [word for word in fw_lower if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RussY09: 805 tokens\n",
      "RussY18: 919 tokens\n"
     ]
    }
   ],
   "source": [
    "# Count the new number of tokens now that stopwords have been removed.\n",
    "print('RussY09: {} tokens'.format(len(RS09_lower_stop)))\n",
    "print('RussY18: {} tokens'.format(len(RS18_lower_stop)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After removing stop words\n",
    "By removing stop words words drop down dramintcally 2009 articles drop 564 from 1369 to 805 tokens and 731 from 1650 to 919 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem the words \n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "        \n",
    "td_stemmed = []\n",
    "for word in td_lower_stop:\n",
    "    stemmed = porter.stem(word)\n",
    "    td_stemmed.append(stemmed)\n",
    "\n",
    "fw_stemmed = []\n",
    "for word in fw_lower_stop:\n",
    "    stemmed = porter.stem(word)\n",
    "    fw_stemmed.append(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters and print out the first 50 words for each corpus.\n",
    "# The RussY09 for the year 2019\n",
    "td_clean = [re.sub(r'[^a-zA-Z0-9]+', '', word) for word in td_stemmed]\n",
    "td_clean = list(filter(None, td_clean))\n",
    "\n",
    "#RussY09 for the year 2018\n",
    "fw_clean = [re.sub(r'[^a-zA-Z0-9]+', '', word) for word in fw_stemmed]\n",
    "fw_clean = list(filter(None, fw_clean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 50 most common words in The Russel_Year_09 are:\n",
      "\n",
      "[('jay', 16), ('make', 16), ('halladay', 15), ('team', 14), ('blue', 13), ('trade', 12), ('better', 10), ('get', 10), ('offer', 8), ('roy', 7), ('would', 7), ('valu', 7), ('two', 7), ('say', 7), ('go', 6), ('game', 6), ('propos', 6), ('it', 6), ('like', 6), ('deal', 5), ('prospect', 5), ('he', 5), ('pick', 5), ('three', 5), ('year', 4), ('market', 4), ('away', 4), ('noth', 4), ('decis', 4), ('accept', 4), ('even', 4), ('fair', 4), ('doesnt', 4), ('guy', 4), ('ye', 4), ('bid', 4), ('basebal', 3), ('ultimatum', 3), ('toronto', 3), ('young', 3), ('name', 3), ('packag', 3), ('alex', 3), ('anthopol', 3), ('pitcher', 3), ('free', 3), ('ed', 3), ('walk', 3), ('probabl', 3), ('nickel', 3)]\n"
     ]
    }
   ],
   "source": [
    "# Use NLTK's FreqDist function to calculate token counts on the cleaned corpora\n",
    "td_freq = nltk.FreqDist(td_clean)\n",
    "fw_freq = nltk.FreqDist(fw_clean)\n",
    "\n",
    "# Print the 50 most common tokens in The Dead...\n",
    "print(\"The 50 most common words in The Russel_Year_09 are:\\n\")\n",
    "print(td_freq.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 50 most common words in Russel_Year_18 Wake are:\n",
      "\n",
      "[('ball', 32), ('fielder', 23), ('left', 18), ('get', 12), ('might', 9), ('outfield', 8), ('batter', 8), ('good', 8), ('defens', 7), ('singl', 7), ('go', 7), ('worth', 7), ('percent', 7), ('one', 6), ('base', 6), ('need', 6), ('wall', 6), ('hit', 6), ('doubl', 6), ('run', 6), ('thing', 5), ('talk', 5), ('cut', 5), ('infield', 5), ('dont', 5), ('make', 5), ('time', 5), ('play', 5), ('way', 5), ('could', 5), ('valu', 5), ('field', 5), ('wer', 5), ('data', 5), ('300', 5), ('bad', 5), ('want', 4), ('it', 4), ('there', 4), ('gap', 4), ('stop', 4), ('fact', 4), ('pick', 4), ('take', 4), ('littl', 4), ('know', 4), ('would', 4), ('case', 4), ('extra', 4), ('ground', 4)]\n"
     ]
    }
   ],
   "source": [
    "# ...and the 50 most common in Russel_Year_18. \n",
    "print(\"The 50 most common words in Russel_Year_18 Wake are:\\n\")\n",
    "print(fw_freq.most_common(50))\n",
    "n_print =50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most common words analysis\n",
    "\n",
    "The 50 words frequency in the year 2009 is all about the team is seems blue jay appears(208) times and Halladay(189) is the main topic about prospect, proposal, and offer. The offer was not fear and someone walkaway. \n",
    "\n",
    "On the other hand, in 2018, it is a different flavor about the skills; the ball appears 349 times left fielder; these always go together in baseball left fielder scores better than the centerfielder. Talking about the batter, the hit, worth, percentage, and also talks about the defense and all components in the baseball game.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the bigram finder and score bigram frequency\n",
    "# We'll first work with The Russel_Year_09\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "td_finder = BigramCollocationFinder.from_words(td_clean)\n",
    "td_scored = td_finder.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('blue', 'jay'), 0.016516516516516516)\n",
      "(('roy', 'halladay'), 0.009009009009009009)\n",
      "(('accept', 'propos'), 0.0045045045045045045)\n",
      "(('alex', 'anthopol'), 0.0045045045045045045)\n",
      "(('draft', 'pick'), 0.0045045045045045045)\n",
      "(('make', 'better'), 0.0045045045045045045)\n",
      "(('make', 'decis'), 0.0045045045045045045)\n",
      "(('say', 'ye'), 0.0045045045045045045)\n",
      "(('two', 'three'), 0.0045045045045045045)\n",
      "(('walk', 'away'), 0.0045045045045045045)\n",
      "(('best', 'offer'), 0.003003003003003003)\n",
      "(('blue', 'jays'), 0.003003003003003003)\n",
      "(('cy', 'young'), 0.003003003003003003)\n",
      "(('decis', 'make'), 0.003003003003003003)\n",
      "(('ed', 'scarborough'), 0.003003003003003003)\n",
      "(('firstround', 'draft'), 0.003003003003003003)\n",
      "(('he', 'go'), 0.003003003003003003)\n",
      "(('jay', 'get'), 0.003003003003003003)\n",
      "(('jay', 'would'), 0.003003003003003003)\n",
      "(('team', 'bid'), 0.003003003003003003)\n",
      "(('three', 'year'), 0.003003003003003003)\n",
      "(('ultimatum', 'game'), 0.003003003003003003)\n",
      "(('valu', 'player'), 0.003003003003003003)\n",
      "(('would', 'accept'), 0.003003003003003003)\n",
      "(('2009', 'flash'), 0.0015015015015015015)\n",
      "(('2010', 'playoff'), 0.0015015015015015015)\n",
      "(('2012', 'like'), 0.0015015015015015015)\n",
      "(('8', '2009'), 0.0015015015015015015)\n",
      "(('995', 'left'), 0.0015015015015015015)\n",
      "(('a', 'carleton'), 0.0015015015015015015)\n",
      "(('abl', 'meet'), 0.0015015015015015015)\n",
      "(('accept', 'though'), 0.0015015015015015015)\n",
      "(('acknowledg', 'jay'), 0.0015015015015015015)\n",
      "(('agent', 'market'), 0.0015015015015015015)\n",
      "(('agent', 'still'), 0.0015015015015015015)\n",
      "(('ahead', 'ed'), 0.0015015015015015015)\n",
      "(('alon', 'fact'), 0.0015015015015015015)\n",
      "(('along', 'give'), 0.0015015015015015015)\n",
      "(('also', 'get'), 0.0015015015015015015)\n",
      "(('also', 'young'), 0.0015015015015015015)\n",
      "(('among', 'team'), 0.0015015015015015015)\n",
      "(('angri', 'not'), 0.0015015015015015015)\n",
      "(('anoth', 'team'), 0.0015015015015015015)\n",
      "(('anthopol', 'cant'), 0.0015015015015015015)\n",
      "(('anthopol', 'make'), 0.0015015015015015015)\n",
      "(('anthopol', 'said'), 0.0015015015015015015)\n",
      "(('appar', 'worth'), 0.0015015015015015015)\n",
      "(('appear', 'normal'), 0.0015015015015015015)\n",
      "(('arent', 'fair'), 0.0015015015015015015)\n",
      "(('argument', 'roy'), 0.0015015015015015015)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the first 50 bigram scores for  The Russel_Year_09\n",
    "for bscore in td_scored[:50]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('left', 'fielder'), 0.023004059539918808)\n",
      "(('cut', 'ball'), 0.005412719891745603)\n",
      "(('make', 'play'), 0.005412719891745603)\n",
      "(('1520', 'percent'), 0.0040595399188092015)\n",
      "(('ball', 'get'), 0.0040595399188092015)\n",
      "(('ball', 'hit'), 0.0040595399188092015)\n",
      "(('ball', 'travel'), 0.0040595399188092015)\n",
      "(('wer', 'talk'), 0.0040595399188092015)\n",
      "(('10point', 'gap'), 0.0027063599458728013)\n",
      "(('ball', 'certainli'), 0.0027063599458728013)\n",
      "(('ball', 'field'), 0.0027063599458728013)\n",
      "(('center', 'fielder'), 0.0027063599458728013)\n",
      "(('could', 'make'), 0.0027063599458728013)\n",
      "(('data', 'set'), 0.0027063599458728013)\n",
      "(('defens', 'metric'), 0.0027063599458728013)\n",
      "(('defens', 'worth'), 0.0027063599458728013)\n",
      "(('fact', 'batter'), 0.0027063599458728013)\n",
      "(('field', 'outfield'), 0.0027063599458728013)\n",
      "(('fielder', 'make'), 0.0027063599458728013)\n",
      "(('front', 'left'), 0.0027063599458728013)\n",
      "(('get', 'wall'), 0.0027063599458728013)\n",
      "(('go', 'way'), 0.0027063599458728013)\n",
      "(('hit', 'ground'), 0.0027063599458728013)\n",
      "(('know', 'ball'), 0.0027063599458728013)\n",
      "(('matt', 'kemp'), 0.0027063599458728013)\n",
      "(('need', 'regress'), 0.0027063599458728013)\n",
      "(('pay', 'attent'), 0.0027063599458728013)\n",
      "(('pick', 'ball'), 0.0027063599458728013)\n",
      "(('play', 'case'), 0.0027063599458728013)\n",
      "(('pretti', 'good'), 0.0027063599458728013)\n",
      "(('regress', 'thing'), 0.0027063599458728013)\n",
      "(('singl', 'doubl'), 0.0027063599458728013)\n",
      "(('statcast', 'data'), 0.0027063599458728013)\n",
      "(('stop', 'ball'), 0.0027063599458728013)\n",
      "(('thing', 'left'), 0.0027063599458728013)\n",
      "(('way', 'wall'), 0.0027063599458728013)\n",
      "(('worth', '1520'), 0.0027063599458728013)\n",
      "(('10', 'bad'), 0.0013531799729364006)\n",
      "(('10', 'good'), 0.0013531799729364006)\n",
      "(('129', 'extra'), 0.0013531799729364006)\n",
      "(('20', 'run'), 0.0013531799729364006)\n",
      "(('2018', 'want'), 0.0013531799729364006)\n",
      "(('250', 'feet'), 0.0013531799729364006)\n",
      "(('28', '2018'), 0.0013531799729364006)\n",
      "(('3', 'run'), 0.0013531799729364006)\n",
      "(('30', 'ball'), 0.0013531799729364006)\n",
      "(('300', 'ball'), 0.0013531799729364006)\n",
      "(('300', 'base'), 0.0013531799729364006)\n",
      "(('300', 'feet'), 0.0013531799729364006)\n",
      "(('300', 'opportun'), 0.0013531799729364006)\n"
     ]
    }
   ],
   "source": [
    "# Print the first 50 bigram scores for Russel_Year_2018\n",
    "fw_finder = BigramCollocationFinder.from_words(fw_clean)\n",
    "fw_scored = fw_finder.score_ngrams(bigram_measures.raw_freq)\n",
    "\n",
    "# Print the first 50 bigram scores for Russel_Year_2018\n",
    "for bscore in fw_scored[:50]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Analysis\n",
    "The bigrams are very interesting how words connect each other, although blue and jay appeared many times, to begin with still in the top one with the word frequency of 0.017% of likelihood ratio and raw frequency. Next is Roy, and the Hallway, machine knew that these words are pair together. Upon looking with the pair of words,  is confirmed the topic between Blue Jays and Roy Hallaway, the writer(Russell) talking if Hallaway will pick two or three persons and seems blue jays walk away from the offer. The writer continued to talk about the blue jay. One that is very noticeable the word better describes in so many ways but has the lowest word frequency.\n",
    "\n",
    "The year 2018 talking about a baseball game norm was the left fielder with a bigram score of 0.023%.  And who make to the about the playoff. Upon looking at 1520, it puzzled me, but later I  figure it out it is 15-20 percent, during the process it got deleted the dash. Base on this data, the writer uses statistics to perform the analysis. One that is very noticeable the word ball describes in so many ways. Besides that, the writer is very connected to the event. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointwise Mutual Information (PMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('roy', 'halladay'), 5.250095350126295)\n",
      "(('blue', 'jay'), 5.138370267567467)\n"
     ]
    }
   ],
   "source": [
    "# Apply a mininmum word frequency of five to The year 2009 on the top 50 bigrams. \n",
    "td_finder.apply_freq_filter(5)\n",
    "td_scored = td_finder.score_ngrams(bigram_measures.pmi)\n",
    "for bscore in td_scored[:50]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('left', 'fielder'), 4.923406437897166)\n"
     ]
    }
   ],
   "source": [
    "# Repeat the same exercise for the year 2018\n",
    "fw_finder.apply_freq_filter(5)\n",
    "fw_scored = fw_finder.score_ngrams(bigram_measures.pmi)\n",
    "for bscore in fw_scored[:50]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PMI Analysis\n",
    "Surprisingly the PMI is very good. Only one word that don't occur together the world 'littil' for the word little in the 2009 article.  All are good collocation pairs have high PMI because the probability of co-occurrence is only slightly lower than the probabilities of occurrence of each word. Conversely, a pair of words whose probabilities of occurrence are considerably higher than their likelihood of co-occurrence gets a small PMI score.\n",
    "\n",
    "Next, using PMI setting a five-word frequency limit. Both articles have 50 qualifying bigrams. These all refer to characters and actions associated with the characters. \n",
    "\n",
    "In the 2009 article, Russell uses a character-centered approach, focusing on the situation itself and how characters react to the same events.\n",
    "\n",
    "On the other hand, the 2018 article. Action-based writing. The author keeps the action flowing.  There are many descriptive bigrams that either involve adjectives, nouns, or adverbs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Two Articles\n",
    "The two articles have a different flavor of the year, month, or day. The author is very focused on both topics. \n",
    "\n",
    "The top 50 words very much are the summary of the story. Next is the value of each word frequency very much the same.\n",
    "\n",
    "Base on two articles, the author changes his writing style base on the event and maintains the efficacy. Upon reading the top 50 words, the writer is has a few emotional peaks. \n",
    "\n",
    "Overall, there aren't many differences between both articles' bigrams. However, there is a difference between corpora, denoting how Russell's writing style evolves from the one story to the next. In 2009 about the controversy on Blue Jay and Halladay. That Blue Jay walks away from the offer, and in the second article is the NBA norm game. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Conducting analysis frequently requires skills and the wisdom to select effective methodologies. Without applying foundational principles for data preparation and associated analysis, could risk time management.\n",
    "\n",
    "NLP is an exciting field for data scientists and engineers. Non-commercial packages and utilities allow coding challenges to be abstracted. Moreover, numerous off-shelf solutions require knowledge to select the right combination of frameworks. Often, the acumen in engineering a problem (or lack of), is noticeable within the associated results. Furthermore, in the area of text mining and NLP, the combination of tokenization and vectorization provide data scientists room for creativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
